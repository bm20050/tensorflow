{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Lab"
   ],
   "metadata": {
    "id": "BLB7sHVoFFvn"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Benchmark class includes various methods that help in constructing a model, training, evaluating its performance, and visualizing the training progress using TensorFlow. \n",
    "\n",
    "Your objective is to implement the MyBenchmark class, which inherits from the Benchmark class and performs simple linear regression.\n",
    "\n",
    "\n",
    "Follow these steps to complete the task:\n",
    "\n",
    "* Inherit the MyBenchmark class from the Benchmark class.\n",
    "\n",
    "* Implement the dataset method to create a simple dataset for linear regression.\n",
    "\n",
    "* Implement the make_hyper_params method to define hyperparameters for the linear regression model.\n",
    "\n",
    "* Implement the make_model method to construct the linear regression model using TensorFlow.\n",
    "\n",
    "* Implement the get_result method to retrieve the training results such as loss.\n",
    "\n",
    "* Initialize an instance of the MyBenchmark class.\n",
    "\n",
    "* Transform the input data using the transform_data method.\n",
    "\n",
    "* Define training parameters, including the number of epochs, batch size, and any required callbacks.\n",
    "\n",
    "* Train and evaluate the linear regression model using the benchmark method.\n",
    "\n",
    "* Visualize the training results, such as loss and predictions, using the provided plotting methods.\n",
    "\n",
    "Additionally, explore and make use of the following advanced features provided by the Benchmark class:\n",
    "\n",
    "* Save and load the trained model using the save_model and load_model methods.\n",
    "* Implement model checkpoints to save the model every N epochs or save the best model using the make_checkpoint_callback method.\n",
    "* Visualize the training progress using TensorBoard by employing the make_tensorboard_callback method."
   ],
   "metadata": {
    "id": "DRVX0ZExFHcG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "metadata": {
    "id": "fn0M6_DWFNJP",
    "ExecuteTime": {
     "start_time": "2023-04-24T11:42:21.467225Z",
     "end_time": "2023-04-24T11:42:36.164793Z"
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "AKfb-dc2E_kT",
    "ExecuteTime": {
     "start_time": "2023-04-24T11:59:09.678383Z",
     "end_time": "2023-04-24T11:59:09.709628Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Benchmark:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def dataset(self, train_size=100):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def min_max_scaler(self, data):\n",
    "        scaler = MinMaxScaler()\n",
    "        return scaler.fit_transform(data)\n",
    "\n",
    "    def standard_scaler(self, data):\n",
    "        scaler = StandardScaler()\n",
    "        return scaler.fit_transform(data)\n",
    "\n",
    "    def transform_data(self, x, y, y_label, scaler):\n",
    "        X = scaler(x)\n",
    "        Y = scaler(y.reshape(-1, 1))\n",
    "        return X, Y\n",
    "\n",
    "    def make_hyper_params(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def make_optimizer(self, name, lr):\n",
    "        print('make_optimizer', name)\n",
    "        if name == 'Adam':\n",
    "            return Adam(learning_rate=lr)\n",
    "        elif name == 'RMSprop':\n",
    "            return RMSprop(learning_rate=lr)\n",
    "        elif name == 'SGD':\n",
    "            return SGD(learning_rate=lr)\n",
    "        else:\n",
    "            raise ValueError('Unknown optimizer')\n",
    "\n",
    "    def make_loss_func(self, name):\n",
    "        if name == 'MAE':\n",
    "            return MeanAbsoluteError()\n",
    "        elif name == 'MSE':\n",
    "            return MeanSquaredError()\n",
    "        else:\n",
    "            raise ValueError('Unknown loss function')\n",
    "\n",
    "    def make_model(self, opt='Adam', lr='0.01', loss_fn='MSE', **kargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def train(self, model, X, y, epochs=50, batch=1, callbacks=None, **kargs):\n",
    "        model.fit(X, y, epochs=epochs, batch_size=batch, callbacks=callbacks)\n",
    "\n",
    "    def evaluate(self, model, X_test, y_test):\n",
    "        return model.evaluate(X_test, y_test)\n",
    "\n",
    "    def predict(self, model, X):\n",
    "        return model.predict(X)\n",
    "\n",
    "    def get_result(self, model, record):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def plot_result(self, data, ylabel, xlabel='epochs'):\n",
    "        plt.plot(data)\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_predict(self, model, X_train, y_true):\n",
    "        y_pred = self.predict(model, X_train)\n",
    "        plt.plot(y_true, label='True')\n",
    "        plt.plot(y_pred, label='Predicted')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def save_model(self, model, filename):\n",
    "        model.save(filename)\n",
    "\n",
    "    def load_model(self, filename):\n",
    "        return tf.keras.models.load_model(filename)\n",
    "\n",
    "    def make_checkpoint_callback(self, save_best_only=True, period=5):\n",
    "        filepath = 'model_checkpoint.h5'\n",
    "        checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, save_best_only=save_best_only,\n",
    "                                                        save_weights_only=False, monitor='val_loss',\n",
    "                                                        mode='min', verbose=1, save_freq='epoch', period=period)\n",
    "        return checkpoint\n",
    "\n",
    "    def make_tensorboard_callback(self, log_dir='./logs'):\n",
    "        tensorboard = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True,\n",
    "                                                     write_images=True,\n",
    "                                                     update_freq='epoch', profile_batch=2, embeddings_freq=1)\n",
    "        return tensorboard\n",
    "\n",
    "    def benchmark(self, X, y, params=None):\n",
    "        import sys\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        hyper_params = self.make_hyper_params() if params is None else params\n",
    "        min_loss = sys.float_info.max\n",
    "        for param in hyper_params:\n",
    "            print('*' * 20)\n",
    "            print('opt: {}, lr: {}, loss_fn: {}, batch: {}' \\\n",
    "                  .format(param['opt'], param['lr'], param['loss_fn'], param['batch']))\n",
    "            model = self.make_model(**param)\n",
    "            record = self.train(model, X_train, y_train, **param)\n",
    "            result = self.get_result(model, record)\n",
    "            print('result', result)\n",
    "            score = self.evaluate(model, X_test, y_test)\n",
    "            if result['loss'][-1] < min_loss:\n",
    "                print(\n",
    "                    'loss: {:.2f}, weights: {}, bias: {}'.format(result['loss'][-1], result['weights'], result['bias']))\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class MyBenchmark(Benchmark):\n",
    "    #  implement your code\n",
    "    def __init__(self, train_size=100):\n",
    "        super().__init__()\n",
    "        self.X, self.y = self.dataset(train_size)\n",
    "\n",
    "    def dataset(self, train_size=100):\n",
    "        X = np.linspace(0, 10, train_size)\n",
    "        y = 3 * X + 5 + np.random.randn(train_size)\n",
    "        return X.reshape(-1, 1), y.reshape(-1, 1)\n",
    "\n",
    "    def make_model(self, opt='Adam', lr='0.01', loss_fn='MSE', **kargs):\n",
    "        model = tf.keras.Sequential(\n",
    "            [tf.keras.layers.Dense(units=1,\n",
    "                                   bias_initializer=tf.keras.initializers.Constant(1), \\\n",
    "                                   input_dim=3)])\n",
    "        #print('opt: {}, lr: {}, loss_fn: {}'.format(opt, lr, loss_fn))\n",
    "        opt = super().make_optimizer(opt, lr)\n",
    "        loss = super().make_loss_func(loss_fn)\n",
    "        model.compile(optimizer=opt, loss=loss)\n",
    "        return model\n",
    "\n",
    "    def transform_data(self, x, y, y_label, scaler):\n",
    "        return super().transform_data(x, y, y_label, scaler)\n",
    "\n",
    "    def make_checkpoint_callback(self, save_best_only=True, period=5):\n",
    "        return super().make_checkpoint_callback(save_best_only, period)\n",
    "\n",
    "    def benchmark(self, X, y, params=None):\n",
    "        import sys\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        hyper_params = self.make_hyper_params() if params is None else params\n",
    "        min_loss = sys.float_info.max\n",
    "        for param in hyper_params:\n",
    "            print('*' * 20)\n",
    "            print('opt: {}, lr: {}, loss_fn: {}, batch: {}' \\\n",
    "                  .format(param['opt'], param['lr'], param['loss_fn'], param['batch']))\n",
    "            model = self.make_model(**param)\n",
    "            record = self.train(model, X_train, y_train, **param)\n",
    "            result = self.get_result(model, record)\n",
    "            print('result', result)\n",
    "            score = self.evaluate(model, X_test, y_test)\n",
    "            if result['loss'][-1] < min_loss:\n",
    "                print(\n",
    "                    'loss: {:.2f}, weights: {}, bias: {}'.format(result['loss'][-1], result['weights'], result['bias']))\n",
    "        return score"
   ],
   "metadata": {
    "id": "Jzvr0dF2FLyZ",
    "ExecuteTime": {
     "start_time": "2023-04-24T11:59:10.279494Z",
     "end_time": "2023-04-24T11:59:10.357627Z"
    }
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Main"
   ],
   "metadata": {
    "id": "aYR5UTm7Fi_G"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "my_benchmark = MyBenchmark(train_size=100)\n",
    "X_transformed, y_transformed = my_benchmark.transform_data(my_benchmark.X, my_benchmark.y, 'Y', my_benchmark.standard_scaler)\n",
    "train_params = [{\n",
    "    'epochs': 50,\n",
    "    'batch': 1,\n",
    "    'loss_fn': 'MAE',\n",
    "    'lr': 0.1,\n",
    "    'opt': 'Adam',\n",
    "    'callbacks': [\n",
    "        my_benchmark.make_checkpoint_callback(save_best_only=True, period=5),\n",
    "        #my_benchmark.make_tensorboard_callback(log_dir='./logs')\n",
    "    ]\n",
    "}]\n",
    "score = my_benchmark.benchmark(X_transformed, y_transformed, train_params)\n",
    "print(score)"
   ],
   "metadata": {
    "id": "hTN90iDHFUGH"
   },
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "********************\n",
      "opt: Adam, lr: 0.1, loss_fn: MAE, batch: 1\n",
      "make_optimizer Adam\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 3), dtype=tf.float32, name='dense_2_input'), name='dense_2_input', description=\"created by layer 'dense_2_input'\"), but it was called on an input with incompatible shape (1, 1).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"D:\\sw15\\tensorflow\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"D:\\sw15\\tensorflow\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"D:\\sw15\\tensorflow\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"D:\\sw15\\tensorflow\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"D:\\sw15\\tensorflow\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"D:\\sw15\\tensorflow\\venv\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 277, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer \"sequential_2\" \"                 f\"(type Sequential).\n    \n    Input 0 of layer \"dense_2\" is incompatible with the layer: expected axis -1 of input shape to have value 3, but received input with shape (1, 1)\n    \n    Call arguments received by layer \"sequential_2\" \"                 f\"(type Sequential):\n      • inputs=tf.Tensor(shape=(1, 1), dtype=float32)\n      • training=True\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 14\u001B[0m\n\u001B[0;32m      2\u001B[0m X_transformed, y_transformed \u001B[38;5;241m=\u001B[39m my_benchmark\u001B[38;5;241m.\u001B[39mtransform_data(my_benchmark\u001B[38;5;241m.\u001B[39mX, my_benchmark\u001B[38;5;241m.\u001B[39my, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mY\u001B[39m\u001B[38;5;124m'\u001B[39m, my_benchmark\u001B[38;5;241m.\u001B[39mstandard_scaler)\n\u001B[0;32m      3\u001B[0m train_params \u001B[38;5;241m=\u001B[39m [{\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mepochs\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m50\u001B[39m,\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbatch\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     12\u001B[0m     ]\n\u001B[0;32m     13\u001B[0m }]\n\u001B[1;32m---> 14\u001B[0m score \u001B[38;5;241m=\u001B[39m \u001B[43mmy_benchmark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbenchmark\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_transformed\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_transformed\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28mprint\u001B[39m(score)\n",
      "Cell \u001B[1;32mIn[19], line 39\u001B[0m, in \u001B[0;36mMyBenchmark.benchmark\u001B[1;34m(self, X, y, params)\u001B[0m\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mopt: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, lr: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, loss_fn: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, batch: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m \\\n\u001B[0;32m     37\u001B[0m       \u001B[38;5;241m.\u001B[39mformat(param[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mopt\u001B[39m\u001B[38;5;124m'\u001B[39m], param[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlr\u001B[39m\u001B[38;5;124m'\u001B[39m], param[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloss_fn\u001B[39m\u001B[38;5;124m'\u001B[39m], param[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbatch\u001B[39m\u001B[38;5;124m'\u001B[39m]))\n\u001B[0;32m     38\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmake_model(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparam)\n\u001B[1;32m---> 39\u001B[0m record \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain(model, X_train, y_train, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparam)\n\u001B[0;32m     40\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_result(model, record)\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mresult\u001B[39m\u001B[38;5;124m'\u001B[39m, result)\n",
      "Cell \u001B[1;32mIn[18], line 47\u001B[0m, in \u001B[0;36mBenchmark.train\u001B[1;34m(self, model, X, y, epochs, batch, callbacks, **kargs)\u001B[0m\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain\u001B[39m(\u001B[38;5;28mself\u001B[39m, model, X, y, epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m, batch\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, callbacks\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkargs):\n\u001B[1;32m---> 47\u001B[0m     \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     48\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
      "File \u001B[1;32mD:\\sw15\\tensorflow\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filedpdqk4mv.py:15\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001B[1;34m(iterator)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     14\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m     retval_ \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(step_function), (ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mself\u001B[39m), ag__\u001B[38;5;241m.\u001B[39mld(iterator)), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[0;32m     17\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: in user code:\n\n    File \"D:\\sw15\\tensorflow\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"D:\\sw15\\tensorflow\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"D:\\sw15\\tensorflow\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"D:\\sw15\\tensorflow\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"D:\\sw15\\tensorflow\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"D:\\sw15\\tensorflow\\venv\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 277, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer \"sequential_2\" \"                 f\"(type Sequential).\n    \n    Input 0 of layer \"dense_2\" is incompatible with the layer: expected axis -1 of input shape to have value 3, but received input with shape (1, 1)\n    \n    Call arguments received by layer \"sequential_2\" \"                 f\"(type Sequential):\n      • inputs=tf.Tensor(shape=(1, 1), dtype=float32)\n      • training=True\n      • mask=None\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
