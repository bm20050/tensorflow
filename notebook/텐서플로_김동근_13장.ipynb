{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNMrYCoDpIU/9MN7tQWrqvC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"-FdYeOV-cYmQ"},"outputs":[],"source":["#13ìž¥ GAN\n","#51_01\n","import tensorflow as tf\n","import numpy as np\n","\n","#1: \n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","tf.config.experimental.set_memory_growth(gpus[0], True)\n","\n","#2: crate a 1D input data\n","A = np.array([1, 2, 3, 4, 5], dtype='float32')\n"," \n","#3: build a model\n","model = tf.keras.Sequential()\n","model.add(tf.keras.layers.Input(shape = (5, 1)))\n","model.add(tf.keras.layers.UpSampling1D())  # size = 2\n","##model.add(tf.keras.layers.Flatten())       # (batch, upsampled_steps*features)\n","model.summary()\n","\n","#4: apply A to model\n","A = np.reshape(A, (1, 5, 1))  # (batch_size, steps, features)\n","output = model.predict(A)     # (batch_size, upsampled_steps, features)\n","B = output.flatten()\n","print(\"B=\", B)\n"]},{"cell_type":"code","source":["#51_02\n","import tensorflow as tf\n","import numpy as np\n","\n","#1: \n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","tf.config.experimental.set_memory_growth(gpus[0], True)\n"," \n","#2: crate a 2D input data\n","A = np.array([[1, 2, 3],\n","              [4, 5, 6]],dtype = 'float32')\n","A = A.reshape(-1, 2, 3, 1)     # (batch, rows, cols, channels)\n","\n","#3: build a model\n","model = tf.keras.Sequential()\n","model.add(tf.keras.layers.Input(A.shape[1:])) # shape=(2, 3, 1)\n","#3-1\n","model.add(tf.keras.layers.UpSampling2D())     # size =(2,2), interpolation='nearest'\n","#3-2\n","##model.add(tf.keras.layers.UpSampling2D(interpolation= 'bilinear')) # size =(2,2)\n","model.summary()\n","\n","#4: apply A to model\n","B = model.predict(A)      # (batch_size, upsampled_rows, upsampled_cols, channels)\n","print(\"B.shape=\", B.shape)\n","print(\"B[0,:,:,0]=\", B[0,:,:,0]) \n"],"metadata":{"id":"z1Yt67_vdRh-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#51_03\n","import tensorflow as tf\n","from tensorflow.keras.datasets import mnist\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","#1: \n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","tf.config.experimental.set_memory_growth(gpus[0], True)\n","\n","#2\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","x_train = x_train.astype('float32')\n","x_test  = x_test.astype('float32')\n","\n","# expand data with channel = 1\n","x_train = np.expand_dims(x_train,axis = 3)      # (60000, 28, 28, 1)\n","x_test  = np.expand_dims(x_test, axis = 3)      # (10000, 28, 28, 1)\n"," \n","#3: build a model\n","model = tf.keras.Sequential()\n","model.add(tf.keras.layers.Input(x_train.shape[1:])) # shape = (28, 28, 1)\n","model.add(tf.keras.layers.UpSampling2D(interpolation='bilinear')) # size =(2,2) \n","model.summary()\n","\n","#4: apply x_train to model\n","output = model.predict(x_train[:8])  # (8, 56, 56, 1)\n","img = output[:,:,:,0]                # 0-channel                  \n","print(\"img.shape=\", img.shape)\n","\n","#5: display images\n","fig = plt.figure(figsize = (8, 4))\n","for i in range(8):   \n","    plt.subplot(2, 4, i + 1)  \n","    plt.imshow(img[i], cmap = 'gray')\n","    plt.axis(\"off\")\n","fig.tight_layout()\n","plt.show()\n"],"metadata":{"id":"Jqw4_lwGdZYY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#52_01\n","import tensorflow as tf\n","import numpy as np\n","\n","#1: \n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","tf.config.experimental.set_memory_growth(gpus[0], True)\n","\n","#2: crate a 2D input data\n","A = np.array([[1, 2],\n","              [3, 4 ]],dtype='float32')\n","A = A.reshape(-1, 2, 2, 1)\n","\n","#3: kernel\n","W = np.array([[ 1,  -1],      \n","               [ 2,  -2]], dtype = 'float32')\n","W = W.reshape(2, 2, 1, 1)   # (kernel_size[0], kernel_size[1], filters, channels)\n","\n","#4: build a model\n","model = tf.keras.Sequential()\n","model.add(tf.keras.layers.Input(A.shape[1:])) # shape = (2, 2, 1)\n","model.add(tf.keras.layers.Conv2DTranspose(filters=1,\n","                                 kernel_size = (2, 2),\n","                                 strides = (1, 1),\n","                                 padding = 'valid',  # 'same'\n","                                 use_bias = False,\n","                                 kernel_initializer = tf.constant_initializer(W)))\n","model.summary()\n","##model.set_weights([W]) # kernel_initializer = tf.constant_initializer(W)\n","\n","#5: apply A to model\n","B = model.predict(A)     # (batch, new_rows, new_cols, filters)\n","print(\"B.shape=\", B.shape)\n","print(\"B[0,:,:,0]=\\n\", B[0,:,:,0])\n","\n","#6: weights\n","##W1 = model.get_weights() # W, model.trainable_variables\n","##print(\"W1[0].shape=\", W1[0].shape)\n","##print(\"W1[0]=\\n\", W1[0])\n"],"metadata":{"id":"Wm6DSKkqdfvT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#52_02\n","import tensorflow as tf\n","import numpy as np\n","\n","#1: \n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","tf.config.experimental.set_memory_growth(gpus[0], True)\n","\n","#2: crate a 2D input data\n","A = np.array([[1, 2],\n","              [3, 4 ]],dtype='float32')\n","A = A.reshape(-1, 2, 2, 1)\n","\n","#3: kernel\n","W = np.array([[ 1,  -1],      \n","              [ 2,  -2]], dtype = 'float32')\n","W = W.reshape(2, 2, 1, 1)   # (kernel_size[0], kernel_size[1], filters, channels)\n","\n","#4: build a model\n","model = tf.keras.Sequential()\n","model.add(tf.keras.layers.Input(A.shape[1:])) # shape = (2, 2, 1)\n","model.add(tf.keras.layers.Conv2DTranspose(filters=1,\n","                                 kernel_size = (2, 2),\n","                                 strides = (2, 2),\n","                                 padding = 'valid',  # 'same'\n","                                 use_bias = False,\n","                                 kernel_initializer = tf.constant_initializer(W)))\n","model.summary()\n","##model.set_weights([W]) # kernel_initializer = tf.constant_initializer(W)\n","\n","#5: apply A to model\n","B = model.predict(A)     # (batch, new_rows, new_cols, filters)\n","print(\"B.shape=\", B.shape)\n","print(\"B[0,:,:,0]=\\n\", B[0,:,:,0])\n","\n","#6: weights\n","##W1 = model.get_weights() # W, model.trainable_variables\n","##print(\"W1[0].shape=\", W1[0].shape)\n","##print(\"W1[0]=\\n\", W1[0])\n"],"metadata":{"id":"F9QxNjuNdl-T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#52_03\n","import tensorflow as tf\n","import numpy as np\n","\n","#1: \n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","tf.config.experimental.set_memory_growth(gpus[0], True)\n","\n","#2: crate a 2D input with 2 channels\n","A = np.array([[[1, 2],           # 0-channel\n","               [3, 4]],\n","              [[1, 2],           # 1-channel \n","               [3, 4]]], dtype = 'float32')\n","##print(\"A.shape\", A.shape)         # (channels, rows, cols) = (2, 2, 2)\n","A = np.transpose(A, (1, 2, 0))      # (rows, cols, channels) = (2, 2, 2)\n","A= np.expand_dims(A, axis = 0)      # (batch,rows, cols, channels)=(1, 2, 2, 2)\n","\n","#3: kernel with 2-channels\n","W = np.array([[[1, -1],           # 0-channel\n","               [2, -2]],\n","              [[1, -1],           # 1-channel \n","               [2, -2]]], dtype = 'float32')\n","##print(\"W.shape\", W.shape)         # (channels, rows, cols) = (2, 2, 2)\n","W = np.transpose(W, (1, 2, 0))      # (rows, cols, channels) = (2, 2, 2)\n","W= np.expand_dims(W, axis = 2)      # (rows, cols, filters, channels) = (2, 2, 1, 2)\n","\n","#4: build a model\n","model = tf.keras.Sequential()\n","model.add(tf.keras.layers.Input(A.shape[1:])) # shape = (2, 2, 2)\n","model.add(tf.keras.layers.Conv2DTranspose(filters=1,\n","                                 kernel_size = (2, 2),\n","                                 strides = (2, 2),\n","                                 padding = 'valid', # 'same          \n","                                 use_bias = False,\n","                                 kernel_initializer = tf.constant_initializer(W)))\n","model.summary()\n","\n","#5: apply A to model\n","B = model.predict(A)     # (batch, new_rows, new_cols, filters)\n","print(\"B.shape=\", B.shape)\n","print(\"B[0,:,:,0]=\\n\", B[0,:,:,0])\n"],"metadata":{"id":"r6KnOUufdtGS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#52_04\n","import tensorflow as tf\n","from tensorflow.keras.datasets import mnist\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","#1: \n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","tf.config.experimental.set_memory_growth(gpus[0], True)\n","\n","#2\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","x_train = x_train.astype('float32')\n","x_test  = x_test.astype('float32')\n","\n","# expand data with channel = 1\n","x_train = np.expand_dims(x_train,axis = 3)      # (60000, 28, 28, 1)\n","x_test  = np.expand_dims(x_test, axis = 3)      # (10000, 28, 28, 1)\n","\n","#3: kernel\n","W = np.array([[ 1,  1],      \n","              [ 1,  1]], dtype = 'float32')\n","W = W.reshape(2, 2, 1, 1)   # (kernel_size[0], kernel_size[1], filters, channels)\n","\n","#4: build a model\n","model = tf.keras.Sequential()\n","model.add(tf.keras.layers.Input(x_train.shape[1:])) # shape = (28, 28, 1)\n","model.add(tf.keras.layers.Conv2DTranspose(filters=1,\n","                                 kernel_size = (2, 2),\n","                                 strides = (2, 2),\n","                                 padding = 'valid',\n","                                 use_bias = False,\n","                                 kernel_initializer = tf.constant_initializer(W)))\n","model.summary()\n","\n","#5: apply x_train to model\n","output = model.predict(x_train[:8])  # (8, 56, 56, 1)\n","img = output[:,:,:,0]                # 0-filters                  \n","print(\"img.shape=\", img.shape)\n","\n","#6: display images\n","fig = plt.figure(figsize = (8, 4))\n","for i in range(8):   \n","    plt.subplot(2, 4, i + 1)  \n","    plt.imshow(img[i], cmap = 'gray')\n","    plt.axis(\"off\")\n","fig.tight_layout()\n","plt.show()\n"],"metadata":{"id":"2SQhEY2hdyHQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#53_01\n","import tensorflow as tf\n","from tensorflow.keras.layers import Input, Dense\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","#1: \n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","tf.config.experimental.set_memory_growth(gpus[0], True)\n","\n","#2: \n","#np.random.seed(1)\n","#X = np.arange(50)\n","#np.random.shuffle(X)\n","#X = X.reshape(-1, 10)\n","X = np.array([[27, 35, 40, 38,  2,  3, 48, 29, 46, 31],\n","              [32, 39, 21, 36, 19, 42, 49, 26, 22, 13],\n","              [41, 17, 45, 24, 23,  4, 33, 14, 30, 10],\n","              [28, 44, 34, 18, 20, 25,  6,  7, 47,  1],\n","              [16,  0, 15,  5, 11,  9,  8, 12, 43, 37]], dtype=np.float)\n","# normalize \n","##A = X/np.max(X)\n","mX = np.mean(X, axis = 0)\n","std = np.std(X, axis = 0)\n","A = (X - mX)/std\n","\n","#3: autoencoder model\n","#3-1:\n","encode_dim = 4  # latent_dim\n","input_x = Input(shape = (10,))  #  A.shape[1:]\n","encode= Dense(units = 8, activation = 'relu')(input_x)\n","encode= Dense(units = encode_dim, activation = 'relu')(encode)\n","encoder = tf.keras.Model(inputs= input_x, outputs= encode)\n","encoder.summary()\n","\n","#3-2:\n","decode_input = Input(shape = (encode_dim,))\n","decode= Dense(units = 8, activation = 'relu')(decode_input)\n","decode= Dense(units = 10, activation = None)(decode)\n","decoder = tf.keras.Model(inputs= decode_input, outputs= decode)\n","decoder.summary\n","\n","#3-3:\n","autoencoder  = tf.keras.Model(inputs = input_x,  outputs = decoder(encoder(input_x)))\n","autoencoder.summary()\n"," \n","#4: train the model\n","opt = tf.keras.optimizers.RMSprop(learning_rate = 0.001) #'rmsprop'\n","autoencoder.compile(optimizer = opt, loss= 'mse', metrics = ['accuracy'])\n","ret = autoencoder.fit(A, A, epochs = 2000, batch_size= 3, verbose=0)\n","\n","#5:\n","x = encoder(A)\n","print(\"x=\\n\", x)\n","\n","B = decoder(x)  # B = autoencoder(A), hat(X) = B*std + mX\n","##print(\"B=\\n\", B)\n","##print(\"A=\\n\", A)  # input\n","print(\"np.abs(A - B)=\\n\", np.abs(A - B))\n","\n","#6:\n","fig, ax = plt.subplots(1, 2, figsize = (10, 6))\n","ax[0].plot(ret.history['loss'], \"g-\")\n","ax[0].set_title(\"train loss\")\n","ax[0].set_xlabel('epochs')\n","ax[0].set_ylabel('loss')\n","\n","ax[1].plot(ret.history['accuracy'], \"b-\")\n","ax[1].set_title(\"accuracy\")\n","ax[1].set_xlabel('epochs')\n","ax[1].set_ylabel('accuracy')\n","fig.tight_layout()\n","plt.show()\n"],"metadata":{"id":"1dGNQFuJd5Nm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#53_02\n","'''\n","ref1: https://towardsdatascience.com/autoencoders-in-keras-c1f57b9a2fd7\n","ref2: https://towardsdatascience.com/how-to-make-an-autoencoder-2f2d99cd5103\n","ref3: https://blog.keras.io/building-autoencoders-in-keras.html\n","'''\n","import tensorflow as tf\n","from tensorflow.keras.datasets import mnist\n","from tensorflow.keras.layers import Input, Dense, Flatten, Reshape\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","#1: \n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","tf.config.experimental.set_memory_growth(gpus[0], True)\n","\n","#2:\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","x_train = x_train.astype('float32')/255\n","x_test  = x_test.astype('float32')/255\n","\n","#3: add noise to dataset\n","x_train_noise = x_train + np.random.normal(loc=0.0, scale=0.2, size=x_train.shape)\n","x_test_noise  = x_test  + np.random.normal(loc=0.0, scale=0.2, size=x_test.shape)\n","x_train_noise = np.clip(x_train_noise, 0, 1)\n","x_test_noise = np.clip(x_test_noise, 0, 1)\n","\n","#4: autoencoder model\n","#4-1:\n","encode_dim = 32 # latent_dim\n","input_x = Input(shape = (28, 28)) #  x_train.shape[1:]\n","encode  = Flatten()(input_x)\n","encode= Dense(units = 64, activation = 'relu')(encode)\n","encode= Dense(units = encode_dim, activation = 'relu')(encode)\n","encoder = tf.keras.Model(inputs= input_x, outputs= encode)\n","##encoder.summary()\n","\n","#4-2:\n","decode_input = Input(shape = (encode_dim,))\n","decode= Dense(units = 64, activation = 'relu')(decode_input)\n","decode= Dense(units = 784, activation ='sigmoid')(decode)\n","decode= Reshape((28, 28))(decode)\n","decoder = tf.keras.Model(inputs= decode_input, outputs= decode)\n","##decoder.summary\n","\n","#4-3:\n","autoencoder  = tf.keras.Model(inputs = input_x,  outputs = decoder(encoder(input_x)))\n","autoencoder.summary()\n"," \n","#5: train the model\n","opt = tf.keras.optimizers.RMSprop(learning_rate = 0.001) #'rmsprop'\n","autoencoder.compile(optimizer = opt, loss= 'mse' ) # 'binary_crossentropy'\n","ret = autoencoder.fit(x=x_train_noise, y=x_train, epochs= 100, batch_size= 128,\n","                       validation_split = 0.2, verbose = 0) \n","\n","#6:\n","fig, ax = plt.subplots(1, 2, figsize = (10, 6))\n","ax[0].plot(ret.history['loss'], \"b-\")\n","ax[0].set_title(\"train loss\")\n","ax[0].set_xlabel('epochs')\n","ax[0].set_ylabel('loss')\n","\n","ax[1].plot(ret.history['val_loss'], \"g-\")\n","ax[1].set_title(\"val loss\")\n","ax[1].set_xlabel('epochs')\n","ax[1].set_ylabel('loss')\n","fig.tight_layout()\n","plt.show()\n","\n","#7: apply  x_test_noise[:8] to model and display\n","F = encoder(x_test_noise[:8])\n","print(\"F.shape=\", F.shape)\n","\n","img = decoder(F)  # img = autoencoder(x_test_noise[:8])\n","print(\"img.shape=\", img.shape)\n"," \n"," \n","#8: display images\n","fig = plt.figure(figsize = (16, 4))\n","for i in range(16):   \n","    plt.subplot(2, 8, i + 1)\n","    if i<8: # noise\n","        plt.imshow(x_test_noise[i], cmap = 'gray')\n","    else:   # reconstructed\n","        plt.imshow(img[i-8], cmap = 'gray')\n","    plt.axis(\"off\")\n","\n","fig.tight_layout()\n","plt.show()\n"],"metadata":{"id":"JMY4JVwKd_EU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#53_03\n","'''\n","ref1:https://towardsdatascience.com/autoencoders-in-keras-c1f57b9a2fd7\n","ref2:\n","https://medium.com/analytics-vidhya/building-a-convolutional-autoencoder-using-keras-using-conv2dtranspose-ca403c8d144e\n","'''\n","import tensorflow as tf\n","from tensorflow.keras.datasets import mnist\n","from tensorflow.keras.layers import Input, Dense,  Flatten, Reshape, BatchNormalization\n","from tensorflow.keras.layers import Conv2D, MaxPool2D, Conv2DTranspose, UpSampling2D\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","#1: \n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","tf.config.experimental.set_memory_growth(gpus[0], True)\n","\n","#2:\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","x_train = x_train.astype('float32')/255\n","x_test  = x_test.astype('float32')/255\n","\n","# expand data with channel = 1\n","x_train = np.expand_dims(x_train,axis = 3)     # (60000, 28, 28, 1)\n","x_test = np.expand_dims(x_test, axis = 3)      # (10000, 28, 28, 1)\n","\n","#3: add noise to dataset\n","x_train_noise = x_train + np.random.normal(loc=0.0, scale=0.2, size=x_train.shape)\n","x_test_noise  = x_test  + np.random.normal(loc=0.0, scale=0.2, size=x_test.shape)\n","x_train_noise = np.clip(x_train_noise, 0, 1)\n","x_test_noise = np.clip(x_test_noise, 0, 1)\n","\n","#4: autoencoder model\n","#4-1:\n","encode_dim = 32  # latent_dim\n","input_x= Input(shape = x_train.shape[1:]) #  (28, 28, 1)\n","encode= Conv2D(filters=32,kernel_size=(3,3), padding='same', activation = 'relu')(input_x)\n","encode= MaxPool2D()(encode) # (14, 14, 32)\n","\n","encode= Conv2D(filters= 16, kernel_size= (3, 3), padding='same', activation = 'relu')(encode)\n","encode= MaxPool2D()(encode) # (7, 7, 16)\n","encode= Flatten()(encode)\n","\n","encode= Dense(units = encode_dim, activation = 'relu')(encode)\n","encoder = tf.keras.Model(inputs= input_x, outputs= encode, name ='encoder')\n","encoder.summary()\n","\n","#4-2: decoder by (Conv2D + UpSampling2D) or Conv2DTranspose\n","decode_input = Input(shape = (encode_dim,))\n","encode= Dense(units = 7*7*4, activation = 'relu')(decode_input)\n","decode= Reshape((7, 7, 4))(encode)\n","\n","##decode=Conv2D(filters=16, kernel_size = (3, 3), strides = (1, 1), \n","##                 activation = 'relu', padding = 'same')(decode)\n","##decode=UpSampling2D()(decode) # size =(2,2)\n","decode=Conv2DTranspose(filters=16, kernel_size = (3, 3), strides = (2, 2), \n","                           activation = 'relu', padding = 'same')(decode)\n","\n","##decode=Conv2D(filters=32, kernel_size = (3, 3), strides = (1, 1), \n","##                 activation = 'relu', padding = 'same')(decode)\n","##decode=UpSampling2D()(decode)\n","decode=Conv2DTranspose(filters=32, kernel_size = (3, 3), strides = (2, 2), \n","                           activation = 'relu', padding = 'same')(decode)\n","\n","decode=Conv2D(filters=1, kernel_size = (3, 3), strides = (1, 1), \n","              activation = 'sigmoid', padding = 'same')(decode)\n","decoder = tf.keras.Model(inputs= decode_input, outputs= decode, name ='decoder')\n","decoder.summary()\n","\n","#4-3\n","autoencoder  = tf.keras.Model(inputs = input_x,\n","                            outputs = decoder(encoder(input_x)), name ='autoencoder')\n","autoencoder.summary()\n"," \n","#5: train the model\n","opt = tf.keras.optimizers.RMSprop(learning_rate = 0.001) #'rmsprop'\n","autoencoder.compile(optimizer = opt, loss= 'mse' ) #  'binary_crossentropy'\n","ret = autoencoder.fit(x=x_train_noise, y=x_train, epochs= 100, batch_size= 128,\n","                       validation_split = 0.2, verbose = 2) \n","\n","#6:\n","##fig, ax = plt.subplots(1, 2, figsize = (10, 6))\n","##ax[0].plot(ret.history['loss'], \"b-\")\n","##ax[0].set_title(\"train loss\")\n","##ax[0].set_xlabel('epochs')\n","##ax[0].set_ylabel('loss')\n","##\n","##ax[1].plot(ret.history['val_loss'], \"g-\")\n","##ax[1].set_title(\"val loss\")\n","##ax[1].set_xlabel('epochs')\n","##ax[1].set_ylabel('loss')\n","##fig.tight_layout()\n","##plt.show()\n","\n","#7: apply  x_test_noise[:8] to model and display\n","F = encoder(x_test_noise[:8])\n","print(\"F.shape=\", F.shape)\n","\n","img = decoder(F)  # img = autoencoder(x_test_noise[:8])\n","img = img.numpy()\n","img = img.reshape(-1, 28, 28)\n","print(\"img.shape=\", img.shape)\n"," \n","#8: display images\n","fig = plt.figure(figsize = (16, 4))\n","for i in range(16):   \n","    plt.subplot(2, 8, i + 1)\n","    if i<8: # noise\n","        plt.imshow(x_test_noise[i,:,:,0], cmap = 'gray')\n","    else:   # reconstructed\n","        plt.imshow(img[i-8], cmap = 'gray')\n","    plt.axis(\"off\")\n","\n","fig.tight_layout()\n","plt.show()\n"],"metadata":{"id":"dZFDonxaeE_6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#54_01\n","'''\n","ref1: https://www.tensorflow.org/tutorials/generative/dcgan?hl=ko\n","ref2: https://github.com/Zackory/Keras-MNIST-GAN/blob/master/mnist_gan.py\n","'''\n","import tensorflow as tf\n","from tensorflow.keras.datasets import mnist\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.layers import Input, Dense, LeakyReLU, Dropout\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","#1: \n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","tf.config.experimental.set_memory_growth(gpus[0], True)\n","\n","#2:\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","x_train = x_train.astype('float32')/127.5 - 1.0  # [ -1, 1]\n","x_train = x_train.reshape(-1, 784)\n","\n","#3: G, D using Sequential\n","noise_dim = 10 # 100\n","\n","#3-1: generator, G\n","##G = Sequential()\n","##G.add(Dense(256, input_dim=noise_dim ))\n","##G.add(LeakyReLU(alpha=0.2))\n","##G.add(Dense(512))\n","##G.add(LeakyReLU(alpha=0.2))\n","##G.add(Dense(1024))\n","##G.add(LeakyReLU(alpha=0.2))\n","##G.add(Dense(784, activation='tanh')) #[-1, 1]\n","##G.compile(loss='binary_crossentropy', optimizer='rmsprop') \n","\n","#3-2:discriminator, D\n","##D = Sequential()\n","##D.add(Dense(1024, input_dim=784))\n","##D.add(LeakyReLU(alpha=0.2))\n","##D.add(Dropout(0.3))\n","\n","##D.add(Dense(512))\n","##D.add(LeakyReLU(alpha=0.2))\n","##D.add(Dropout(0.3))\n","##D.add(Dense(256))\n","##D.add(LeakyReLU(alpha=0.2))\n","##D.add(Dropout(0.3))\n","##D.add(Dense(1, activation='sigmoid'))\n","##D.compile(loss='binary_crossentropy', optimizer='rmsprop')\n","\n","#4: G, D using Model\n","noise_dim = 10 # 100\n","#4-1\n","g_input = Input(shape = (noise_dim, ))\n","x= Dense(units = 256)(g_input)\n","x= LeakyReLU(alpha=0.2)(x)\n","x= Dense(units = 512)(x)\n","x= LeakyReLU(alpha=0.2)(x)\n","x= Dense(units = 1024)(x)\n","x= LeakyReLU(alpha=0.2)(x)\n","g_out= Dense(784, activation='tanh')(x) # [-1, 1]\n","G = Model(inputs= g_input, outputs= g_out, name=\"G\")\n","G.summary()\n","##G.compile(loss='binary_crossentropy', optimizer='rmsprop')\n","\n","\n","#4-2: discriminator, D\n","d_input = Input(shape = (784, ))\n","x= Dense(units = 1024)(d_input)\n","x= LeakyReLU()(x)\n","x= Dropout(0.3)(x)\n","\n","x= Dense(units = 512)(x)\n","x= LeakyReLU()(x)\n","x= Dropout(0.3)(x)\n","\n","x= Dense(units = 256)(x)\n","x= LeakyReLU()(x)\n","x= Dropout(0.3)(x)\n","d_out= Dense(1, activation='sigmoid')(x)\n","D = Model(inputs= d_input, outputs= d_out, name=\"D\")\n","D.summary()\n","##D.compile(loss='binary_crossentropy', optimizer='rmsprop')\n","\n","#5: GAN\n","##D.trainable = False\n","gan_input = Input(shape=(noise_dim,))\n","x = G(gan_input)\n","gan_output = D(x)\n","GAN = Model(inputs=gan_input, outputs=gan_output, name=\"GAN\")\n","GAN.summary()\n","##gan.compile(loss='binary_crossentropy', optimizer='rmsprop')\n","\n","#6\n","batch_size = 4\n","noise = tf.random.normal([batch_size, noise_dim])\n","fake = G(noise)\n","out = D(fake)   # out= D(G(noise)), GAN(noise), out= GAN.predict(noise)\n","print('out=', out)\n","##print('GAN(noise)=', GAN(noise))\n","##print('D(x_train[:batch_size])=', D(x_train[:batch_size]))\n","\n","fig = plt.figure(figsize = (8, 2))\n","for i in range(batch_size):\n","    plt.subplot(1, 4, i + 1)  \n","    plt.imshow(fake[i].numpy().reshape((28, 28)), cmap = 'gray')\n","    plt.axis(\"off\")\n","fig.tight_layout()\n","plt.show()\n"],"metadata":{"id":"OPj9nCKAeQ9U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#54_02\n","'''\n","ref1: https://www.tensorflow.org/tutorials/generative/dcgan?hl=ko\n","ref2: https://github.com/Zackory/Keras-MNIST-GAN/blob/master/mnist_gan.py\n","'''\n","import tensorflow as tf\n","from tensorflow.keras.datasets import mnist\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.layers import Input, Dense, Flatten, Reshape, LeakyReLU, Dropout\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","#1: \n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","tf.config.experimental.set_memory_growth(gpus[0], True)\n","\n","#2:\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","x_train = x_train.astype('float32')/127.5 - 1.0  # [ -1, 1]\n","x_train = x_train.reshape(-1, 784)\n","\n","##opt = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1 = 0.5)\n","opt = tf.keras.optimizers.RMSprop(learning_rate = 0.0002)\n","\n","#3: create model \n","#3-1: generator, G\n","noise_dim = 100\n","g_input = Input(shape = (noise_dim, ))\n","x= Dense(units = 256)(g_input)\n","x= LeakyReLU(alpha=0.2)(x)\n","x= Dense(units = 512)(x)\n","x= LeakyReLU(alpha=0.2)(x)\n","x= Dense(units = 1024)(x)\n","x= LeakyReLU(alpha=0.2)(x)\n","g_out= Dense(784, activation='tanh')(x) # [-1, 1]\n","G = Model(inputs= g_input, outputs= g_out, name=\"G\")\n","G.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n","##G.summary()\n","\n","#3-2: discriminator, D\n","d_input = Input(shape = (784, ))\n","x= Dense(units = 1024)(d_input)\n","x= LeakyReLU()(x)\n","x= Dropout(0.3)(x)\n","x= Dense(units = 512)(x)\n","x= LeakyReLU()(x)\n","x= Dropout(0.3)(x)\n","x= Dense(units = 256)(x)\n","x= LeakyReLU()(x)\n","x= Dropout(0.3)(x)\n","d_out= Dense(1, activation='sigmoid')(x)\n","D = Model(inputs= d_input, outputs= d_out, name=\"D\")\n","D.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n","##D.summary() # In model D, D.trainable = True is fixed by D.compile()\n","\n","#3-3: GAN model\n","D.trainable = False \n","gan_input = Input(shape=(noise_dim,))\n","DCGAN = Model(inputs=gan_input, outputs=D(G(gan_input)), name=\"GAN\")\n","DCGAN.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n","##GAN.summary() # In GAN, D.trainable = False is fixed by GAN.compile()\n","\n","#4:\n","import os\n","if not os.path.exists(\"./GAN\"):\n","     os.mkdir(\"./GAN\")\n","def plotGeneratedImages(epoch, examples=20, dim=(2, 10), figsize=(10, 2)):\n","    noise = np.random.normal(0, 1, size=[examples, noise_dim])\n","    g_image = G.predict(noise)\n","    g_image = g_image.reshape(examples, 28, 28)\n","    g_image = (g_image + 1.0)*127.5\n","    g_image = g_image.astype('uint8')\n","\n","    plt.figure(figsize=figsize)\n","    for i in range(g_image.shape[0]):\n","        plt.subplot(dim[0], dim[1], i+1)\n","        plt.imshow(g_image[i], cmap='gray')\n","        plt.axis('off')\n","    plt.tight_layout()\n","    plt.savefig(\"./GAN/gan_epoch_%d.png\"% epoch)\n","    plt.close()\n","      \n","#5:\n","BUFFER_SIZE = x_train.shape[0] # 60000\n","BATCH_SIZE  = 128\n","batch_count = np.ceil(BUFFER_SIZE/BATCH_SIZE)\n","train_dataset = tf.data.Dataset.from_tensor_slices(x_train).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n","\n","history = {\"g_loss\":[], \"g_acc\":[], \"d_loss\":[], \"d_acc\":[]}\n","def train(epochs=100):\n","    for epoch in range(epochs):\n","        dloss = 0.0\n","        gloss = 0.0\n","        dacc  = 0.0\n","        gacc  = 0.0\n","\n","        for batch in train_dataset:  # batch.shape = (BATCH_SIZE, 784)\n","            batch_size = batch.shape[0]\n","            noise = tf.random.normal([batch_size, noise_dim])\n","            fake = G.predict(noise)  # fake.shape = (batch_size, 784)\n","            X = np.concatenate([batch, fake]) # X.shape = (2*batch_size, 784)     \n","\n","            # labels for fake = 0, real(batch) = 1\n","            y_dis = np.zeros(2*batch_size)\n","            y_dis[:batch_size] = 1.0\n","\n","            # train discriminator, D\n","            ret = D.train_on_batch(X, y_dis) # D.trainable = True\n","            dloss += ret[0] # loss\n","            dacc  += ret[1] # accuracy\n","            \n","            # train generator, G\n","            noise = tf.random.normal([batch_size, noise_dim])\n","            y_gen = np.ones(batch_size)\n","            ret= DCGAN.train_on_batch(noise, y_gen) # D.trainable = False\n","            gloss += ret[0]\n","            gacc  += ret[1]\n","\n","        avg_gloss = gloss/batch_count\n","        avg_gacc  = gacc/batch_count     \n","        avg_dloss = dloss/batch_count\n","        avg_dacc  = dacc/batch_count\n","    \n","        print(\"epoch={}: G:(loss= {:.4f}, acc={:.1f}), D:(loss= {:.4f}, acc={:.1f})\".format(epoch, avg_gloss,100*avg_gacc, avg_dloss, 100*avg_dacc))\n","        history[\"g_loss\"].append(avg_gloss)\n","        history[\"g_acc\"].append(avg_gacc)\n","        history[\"d_loss\"].append(avg_dloss)\n","        history[\"d_acc\"].append(avg_dacc)\n","   \n","        if epoch % 20 == 0 or epoch == epochs-1:\n","            plotGeneratedImages(epoch)\n","train()\n","   \n","#6:\n","fig, ax = plt.subplots(1, 2, figsize = (10, 6))\n","ax[0].plot(history[\"g_loss\"], \"g-\", label = \"G losses\")\n","ax[0].plot(history[\"d_loss\"], \"b-\", label = \"D losses\")\n","ax[0].set_title(\"train loss\")\n","ax[0].set_xlabel(\"epochs\")\n","ax[0].set_ylabel(\"loss\")\n","ax[0].legend()\n","\n","ax[1].plot(history[\"g_acc\"], \"g-\",  label = \"G accuracy\")\n","ax[1].plot(history[\"d_acc\"], \"b-\",  label = \"D accuracy\")\n","ax[1].set_title(\"accuracy\")\n","ax[1].set_xlabel(\"epochs\")\n","ax[1].set_ylabel(\"accuracy\")\n","ax[1].legend()\n","fig.tight_layout()\n","plt.show()\n"],"metadata":{"id":"bIuGEIH_eV-6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#54_03\n","'''\n","ref1: https://www.tensorflow.org/tutorials/generative/dcgan?hl=ko\n","ref2: https://github.com/Zackory/Keras-MNIST-GAN/blob/master/mnist_dcgan.py\n","'''\n","import tensorflow as tf\n","from tensorflow.keras.datasets import mnist\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.layers import Input, Dense, Flatten, Reshape, LeakyReLU, Dropout\n","from tensorflow.keras.layers import Conv2D, Conv2DTranspose, BatchNormalization\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","#1: \n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","tf.config.experimental.set_memory_growth(gpus[0], True)\n","\n","#2:\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","x_train = x_train.astype('float32')/127.5 - 1.0  # [ -1, 1]\n","x_train = np.expand_dims(x_train, axis=3)        # (60000, 28, 28, 1)\n","\n","opt = tf.keras.optimizers.RMSprop(learning_rate = 0.0002)\n","##opt = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1 = 0.5)\n","\n","##init_lr = 0.0002\n","##lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n","##              init_lr, decay_steps=469*10*2, decay_rate=0.96, staircase=True)\n","##opt = tf.keras.optimizers.Adam(learning_rate = lr_schedule)\n","\n","#3: create model \n","#3-1: generator, G\n","noise_dim = 100\n","g_input = Input(shape = (noise_dim, ))\n","x= Dense(units = 7*7*128, activation = 'relu')(g_input)\n","x= Reshape((7, 7, 128))(x)\n","x= Conv2DTranspose(filters=64, kernel_size = (3, 3), strides = (2, 2), \n","                   activation = 'relu', padding = 'same')(x)\n","x= BatchNormalization()(x)\n","x= Conv2DTranspose(filters=32, kernel_size = (3, 3), strides = (2, 2), \n","                   activation = 'relu', padding = 'same')(x)\n","x= BatchNormalization()(x)\n","\n","g_output= Conv2D(filters=1, kernel_size = (3, 3), strides = (1, 1), \n","              activation = 'tanh', padding = 'same')(x) # (None, 28, 28, 1)\n","G= Model(inputs= g_input, outputs= g_output, name ='G')\n","G.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n","G.summary()\n","\n","#3-2: discriminator, D\n","d_input = Input(shape = (28, 28, 1))\n","x= Conv2D(32, kernel_size=3, strides=2, padding=\"same\")(d_input)\n","x= LeakyReLU()(x)\n","x= Dropout(0.3)(x)\n","\n","x = Conv2D(64, kernel_size=3, strides=2, padding=\"same\")(x) \n","x= LeakyReLU()(x)\n","x= Dropout(0.3)(x)\n","\n","x = Conv2D(128, kernel_size=3, strides=2, padding=\"same\")(x) \n","x= LeakyReLU()(x)\n","x= Dropout(0.3)(x)\n","\n","x = Flatten()(x)\n","d_output = Dense(1, activation='sigmoid')(x)\n","\n","D = Model(inputs= d_input, outputs= d_output, name=\"D\")\n","D.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n","D.summary()\n","\n","#3-3: GAN model\n","D.trainable = False \n","gan_input = Input(shape=(noise_dim,))\n","DCGAN = Model(inputs=gan_input, outputs=D(G(gan_input)), name=\"GAN\")\n","DCGAN.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n","DCGAN.summary()\n","\n","#4:\n","import os\n","if not os.path.exists(\"./GAN\"):\n","     os.mkdir(\"./GAN\")\n","\n","def plotGeneratedImages(epoch, examples=20, dim=(2, 10), figsize=(10, 2)):\n","    noise = np.random.normal(0, 1, size=[examples, noise_dim])\n","    g_image = G.predict(noise)\n","    g_image = np.squeeze(g_image, axis = 3) \n","\n","    g_image = (g_image + 1.0)*127.5\n","    g_image = g_image.astype('uint8')\n","\n","    plt.figure(figsize=figsize)\n","    for i in range(g_image.shape[0]):\n","        plt.subplot(dim[0], dim[1], i+1)\n","        plt.imshow(g_image[i], cmap='gray')\n","        plt.axis('off')\n","    plt.tight_layout()\n","    plt.savefig(\"./GAN/dcgan_epoch_%d.png\"% epoch)\n","    plt.close()\n","      \n","#5:\n","BUFFER_SIZE = x_train.shape[0] # 60000\n","BATCH_SIZE  = 128\n","batch_count = np.ceil(BUFFER_SIZE/BATCH_SIZE)\n","train_dataset = tf.data.Dataset.from_tensor_slices(x_train).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n","\n","history = {\"g_loss\":[], \"g_acc\":[], \"d_loss\":[], \"d_acc\":[]}\n","def train(epochs=100):\n","\n","    for epoch in range(epochs):\n","        dloss = 0.0\n","        gloss = 0.0\n","        dacc  = 0.0\n","        gacc  = 0.0\n","\n","##      batch = x_train[np.random.randint(0, x_train.shape[0], size=batch_size)]\n","##      print(\"epoch = \", D.optimizer._decayed_lr('float32').numpy())\n","\n","        for batch in train_dataset:  # batch.shape = (BATCH_SIZE, 28, 28, 1)\n","            batch_size = batch.shape[0]\n","            \n","            noise = tf.random.normal([batch_size, noise_dim])\n","            fake = G.predict(noise)  # fake.shape = (batch_size, 784)\n","            X = np.concatenate([batch, fake]) # X.shape = (2*batch_size, 784)     \n","\n","            # labels for fake = 0, batch = 1\n","            y_dis = np.zeros(2*batch_size)\n","            y_dis[:batch_size] = 1.0\n","\n","            # train discriminator, D\n","            ret = D.train_on_batch(X, y_dis) # D.trainable = True\n","            dloss += ret[0] # loss\n","            dacc  += ret[1] # accuracy\n","            \n","            # train generator, G\n","            noise = tf.random.normal([batch_size, noise_dim])\n","            y_gen = np.ones(batch_size)\n","            ret= DCGAN.train_on_batch(noise, y_gen) # D.trainable = False\n","            gloss += ret[0]\n","            gacc  += ret[1]\n","\n","        avg_gloss = gloss/batch_count\n","        avg_gacc  = gacc/batch_count\n","        \n","        avg_dloss = dloss/batch_count\n","        avg_dacc  = dacc/batch_count\n","\n","    \n","        print(\"epoch={}: G:(loss= {:.4f}, acc={:.1f}), D:(loss= {:.4f}, acc={:.1f})\".format(\n","            epoch, avg_gloss,100*avg_gacc, avg_dloss, 100*avg_dacc))\n","        history[\"g_loss\"].append(avg_gloss)\n","        history[\"g_acc\"].append(avg_gacc)\n","        history[\"d_loss\"].append(avg_dloss)\n","        history[\"d_acc\"].append(avg_dacc)\n","   \n","        if epoch % 20 == 0 or epoch == epochs-1:\n","            plotGeneratedImages(epoch)\n","train(100)\n","   \n","#6:\n","fig, ax = plt.subplots(1, 2, figsize = (10, 6))\n","ax[0].plot(history[\"g_loss\"], \"g-\", label = \"G losses\")\n","ax[0].plot(history[\"d_loss\"], \"b-\", label = \"D losses\")\n","ax[0].set_title(\"train loss\")\n","ax[0].set_xlabel(\"epochs\")\n","ax[0].set_ylabel(\"loss\")\n","ax[0].legend()\n","\n","ax[1].plot(history[\"g_acc\"], \"g-\",  label = \"G accuracy\")\n","ax[1].plot(history[\"d_acc\"], \"b-\",  label = \"D accuracy\")\n","ax[1].set_title(\"accuracy\")\n","ax[1].set_xlabel(\"epochs\")\n","ax[1].set_ylabel(\"accuracy\")\n","ax[1].legend()\n","fig.tight_layout()\n","plt.show()\n"],"metadata":{"id":"_E58fcn1eboy"},"execution_count":null,"outputs":[]}]}